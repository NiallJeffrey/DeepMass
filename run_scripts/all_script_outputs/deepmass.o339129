[33mWARNING: Non existent mount point (file) in container: '/var/singularity/mnt/final/usr/bin/nvidia-debugdump'
[0m[33mWARNING: Non existent mount point (file) in container: '/var/singularity/mnt/final/usr/bin/nvidia-persistenced'
[0m[33mWARNING: Non existent mount point (file) in container: '/var/singularity/mnt/final/usr/bin/nvidia-cuda-mps-control'
[0m[33mWARNING: Non existent mount point (file) in container: '/var/singularity/mnt/final/usr/bin/nvidia-cuda-mps-server'
[0m/home/ucapnje
/home/ucapnje/share/DeepMass/run_scripts
loading mask 

(256, 256)
loading data:
Loading /home/ucapnje/share/DeepMass/run_scripts/../picola_training/nicaea_rescaled/training_data00/sv_training_kappa_true.npy
Loading /home/ucapnje/share/DeepMass/run_scripts/../picola_training/nicaea_rescaled/training_data01/sv_training_kappa_true.npy
Loading /home/ucapnje/share/DeepMass/run_scripts/../picola_training/nicaea_rescaled/training_data02/sv_training_kappa_true.npy
Loading /home/ucapnje/share/DeepMass/run_scripts/../picola_training/nicaea_rescaled/training_data03/sv_training_kappa_true.npy
Loading /home/ucapnje/share/DeepMass/run_scripts/../picola_training/nicaea_rescaled/training_data04/sv_training_kappa_true.npy
Loading /home/ucapnje/share/DeepMass/run_scripts/../picola_training/nicaea_rescaled/training_data05/sv_training_kappa_true.npy
Loading /home/ucapnje/share/DeepMass/run_scripts/../picola_training/nicaea_rescaled/training_data06/sv_training_kappa_true.npy
Loading /home/ucapnje/share/DeepMass/run_scripts/../picola_training/nicaea_rescaled/training_data07/sv_training_kappa_true.npy
Loading /home/ucapnje/share/DeepMass/run_scripts/../picola_training/nicaea_rescaled/training_data08/sv_training_kappa_true.npy
Loading /home/ucapnje/share/DeepMass/run_scripts/../picola_training/nicaea_rescaled/training_data09/sv_training_kappa_true.npy
Loading /home/ucapnje/share/DeepMass/run_scripts/../picola_training/nicaea_rescaled/training_data10/sv_training_kappa_true.npy
Loading /home/ucapnje/share/DeepMass/run_scripts/../picola_training/nicaea_rescaled/training_data11/sv_training_kappa_true.npy
Loading /home/ucapnje/share/DeepMass/run_scripts/../picola_training/nicaea_rescaled/training_data12/sv_training_kappa_true.npy
Loading /home/ucapnje/share/DeepMass/run_scripts/../picola_training/nicaea_rescaled/training_data13/sv_training_kappa_true.npy
Loading /home/ucapnje/share/DeepMass/run_scripts/../picola_training/nicaea_rescaled/training_data14/sv_training_kappa_true.npy
Loading /home/ucapnje/share/DeepMass/run_scripts/../picola_training/nicaea_rescaled/training_data15/sv_training_kappa_true.npy
Loading /home/ucapnje/share/DeepMass/run_scripts/../picola_training/nicaea_rescaled/training_data16/sv_training_kappa_true.npy
Loading /home/ucapnje/share/DeepMass/run_scripts/../picola_training/nicaea_rescaled/training_data17/sv_training_kappa_true.npy
Loading /home/ucapnje/share/DeepMass/run_scripts/../picola_training/nicaea_rescaled/training_data18/sv_training_kappa_true.npy
Loading /home/ucapnje/share/DeepMass/run_scripts/../picola_training/nicaea_rescaled/training_data19/sv_training_kappa_true.npy
Loading /home/ucapnje/share/DeepMass/run_scripts/../picola_training/nicaea_rescaled/training_data20/sv_training_kappa_true.npy
Loading /home/ucapnje/share/DeepMass/run_scripts/../picola_training/nicaea_rescaled/training_data21/sv_training_kappa_true.npy
Loading /home/ucapnje/share/DeepMass/run_scripts/../picola_training/nicaea_rescaled/training_data22/sv_training_kappa_true.npy
Loading /home/ucapnje/share/DeepMass/run_scripts/../picola_training/nicaea_rescaled/training_data23/sv_training_kappa_true.npy
Loading /home/ucapnje/share/DeepMass/run_scripts/../picola_training/nicaea_rescaled/training_data24/sv_training_kappa_true.npy
Loading /home/ucapnje/share/DeepMass/run_scripts/../picola_training/nicaea_rescaled/training_data25/sv_training_kappa_true.npy
Loading /home/ucapnje/share/DeepMass/run_scripts/../picola_training/nicaea_rescaled/training_data26/sv_training_kappa_true.npy
Loading /home/ucapnje/share/DeepMass/run_scripts/../picola_training/nicaea_rescaled/training_data27/sv_training_kappa_true.npy
Loading /home/ucapnje/share/DeepMass/run_scripts/../picola_training/nicaea_rescaled/training_data28/sv_training_kappa_true.npy
Loading /home/ucapnje/share/DeepMass/run_scripts/../picola_training/nicaea_rescaled/training_data29/sv_training_kappa_true.npy
Loading /home/ucapnje/share/DeepMass/run_scripts/../picola_training/nicaea_rescaled/training_data30/sv_training_kappa_true.npy
Loading /home/ucapnje/share/DeepMass/run_scripts/../picola_training/nicaea_rescaled/training_data31/sv_training_kappa_true.npy
Loading /home/ucapnje/share/DeepMass/run_scripts/../picola_training/nicaea_rescaled/training_data32/sv_training_kappa_true.npy
Loading /home/ucapnje/share/DeepMass/run_scripts/../picola_training/nicaea_rescaled/training_data33/sv_training_kappa_true.npy
Loading /home/ucapnje/share/DeepMass/run_scripts/../picola_training/nicaea_rescaled/training_data34/sv_training_kappa_true.npy
Loading /home/ucapnje/share/DeepMass/run_scripts/../picola_training/nicaea_rescaled/training_data35/sv_training_kappa_true.npy
Loading /home/ucapnje/share/DeepMass/run_scripts/../picola_training/nicaea_rescaled/training_data36/sv_training_kappa_true.npy
Loading /home/ucapnje/share/DeepMass/run_scripts/../picola_training/nicaea_rescaled/training_data37/sv_training_kappa_true.npy
Loading /home/ucapnje/share/DeepMass/run_scripts/../picola_training/nicaea_rescaled/training_data38/sv_training_kappa_true.npy
Loading /home/ucapnje/share/DeepMass/run_scripts/../picola_training/nicaea_rescaled/training_data39/sv_training_kappa_true.npy
Loading /home/ucapnje/share/DeepMass/run_scripts/../picola_training/nicaea_rescaled/training_data40/sv_training_kappa_true.npy
Loading /home/ucapnje/share/DeepMass/run_scripts/../picola_training/nicaea_rescaled/training_data41/sv_training_kappa_true.npy
Loading /home/ucapnje/share/DeepMass/run_scripts/../picola_training/nicaea_rescaled/training_data42/sv_training_kappa_true.npy
Loading /home/ucapnje/share/DeepMass/run_scripts/../picola_training/nicaea_rescaled/training_data43/sv_training_kappa_true.npy
Loading /home/ucapnje/share/DeepMass/run_scripts/../picola_training/nicaea_rescaled/training_data44/sv_training_kappa_true.npy
Loading /home/ucapnje/share/DeepMass/run_scripts/../picola_training/nicaea_rescaled/training_data45/sv_training_kappa_true.npy
Loading /home/ucapnje/share/DeepMass/run_scripts/../picola_training/nicaea_rescaled/training_data46/sv_training_kappa_true.npy
Loading /home/ucapnje/share/DeepMass/run_scripts/../picola_training/nicaea_rescaled/training_data47/sv_training_kappa_true.npy
Loading /home/ucapnje/share/DeepMass/run_scripts/../picola_training/nicaea_rescaled/training_data48/sv_training_kappa_true.npy
Loading /home/ucapnje/share/DeepMass/run_scripts/../picola_training/nicaea_rescaled/training_data49/sv_training_kappa_true.npy
Loading /home/ucapnje/share/DeepMass/run_scripts/../picola_training/nicaea_rescaled/training_data50/sv_training_kappa_true.npy
Loading /home/ucapnje/share/DeepMass/run_scripts/../picola_training/nicaea_rescaled/training_data51/sv_training_kappa_true.npy
Loading /home/ucapnje/share/DeepMass/run_scripts/../picola_training/nicaea_rescaled/training_data52/sv_training_kappa_true.npy
Loading /home/ucapnje/share/DeepMass/run_scripts/../picola_training/nicaea_rescaled/training_data53/sv_training_kappa_true.npy
Loading /home/ucapnje/share/DeepMass/run_scripts/../picola_training/nicaea_rescaled/training_data54/sv_training_kappa_true.npy
Loading /home/ucapnje/share/DeepMass/run_scripts/../picola_training/nicaea_rescaled/training_data55/sv_training_kappa_true.npy
Loading /home/ucapnje/share/DeepMass/run_scripts/../picola_training/nicaea_rescaled/training_data56/sv_training_kappa_true.npy
Loading /home/ucapnje/share/DeepMass/run_scripts/../picola_training/nicaea_rescaled/training_data57/sv_training_kappa_true.npy
2661.5389692783356
loading noisy data
Loading /home/ucapnje/share/DeepMass/run_scripts/../picola_training/nicaea_rescaled/training_data00/sv_training_wiener.npy
Loading /home/ucapnje/share/DeepMass/run_scripts/../picola_training/nicaea_rescaled/training_data01/sv_training_wiener.npy
Loading /home/ucapnje/share/DeepMass/run_scripts/../picola_training/nicaea_rescaled/training_data02/sv_training_wiener.npy
Loading /home/ucapnje/share/DeepMass/run_scripts/../picola_training/nicaea_rescaled/training_data03/sv_training_wiener.npy
Loading /home/ucapnje/share/DeepMass/run_scripts/../picola_training/nicaea_rescaled/training_data04/sv_training_wiener.npy
Loading /home/ucapnje/share/DeepMass/run_scripts/../picola_training/nicaea_rescaled/training_data05/sv_training_wiener.npy2019-06-17 23:43:54.693951: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA
2019-06-17 23:43:55.895003: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1405] Found device 0 with properties: 
name: Tesla V100-PCIE-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.38
pciBusID: 0000:3b:00.0
totalMemory: 15.75GiB freeMemory: 15.44GiB
2019-06-17 23:43:55.895119: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0
2019-06-17 23:43:58.982179: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-06-17 23:43:58.982225: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971]      0 
2019-06-17 23:43:58.982232: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 0:   N 
2019-06-17 23:43:58.982358: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14941 MB memory) -> physical GPU (device: 0, name: Tesla V100-PCIE-16GB, pci bus id: 0000:3b:00.0, compute capability: 7.0)

Loading /home/ucapnje/share/DeepMass/run_scripts/../picola_training/nicaea_rescaled/training_data06/sv_training_wiener.npy
Loading /home/ucapnje/share/DeepMass/run_scripts/../picola_training/nicaea_rescaled/training_data07/sv_training_wiener.npy
Loading /home/ucapnje/share/DeepMass/run_scripts/../picola_training/nicaea_rescaled/training_data08/sv_training_wiener.npy
Loading /home/ucapnje/share/DeepMass/run_scripts/../picola_training/nicaea_rescaled/training_data09/sv_training_wiener.npy
Loading /home/ucapnje/share/DeepMass/run_scripts/../picola_training/nicaea_rescaled/training_data10/sv_training_wiener.npy
Loading /home/ucapnje/share/DeepMass/run_scripts/../picola_training/nicaea_rescaled/training_data11/sv_training_wiener.npy
Loading /home/ucapnje/share/DeepMass/run_scripts/../picola_training/nicaea_rescaled/training_data12/sv_training_wiener.npy
Loading /home/ucapnje/share/DeepMass/run_scripts/../picola_training/nicaea_rescaled/training_data13/sv_training_wiener.npy
Loading /home/ucapnje/share/DeepMass/run_scripts/../picola_training/nicaea_rescaled/training_data14/sv_training_wiener.npy
Loading /home/ucapnje/share/DeepMass/run_scripts/../picola_training/nicaea_rescaled/training_data15/sv_training_wiener.npy
Loading /home/ucapnje/share/DeepMass/run_scripts/../picola_training/nicaea_rescaled/training_data16/sv_training_wiener.npy
Loading /home/ucapnje/share/DeepMass/run_scripts/../picola_training/nicaea_rescaled/training_data17/sv_training_wiener.npy
Loading /home/ucapnje/share/DeepMass/run_scripts/../picola_training/nicaea_rescaled/training_data18/sv_training_wiener.npy
Loading /home/ucapnje/share/DeepMass/run_scripts/../picola_training/nicaea_rescaled/training_data19/sv_training_wiener.npy
Loading /home/ucapnje/share/DeepMass/run_scripts/../picola_training/nicaea_rescaled/training_data20/sv_training_wiener.npy
Loading /home/ucapnje/share/DeepMass/run_scripts/../picola_training/nicaea_rescaled/training_data21/sv_training_wiener.npy
Loading /home/ucapnje/share/DeepMass/run_scripts/../picola_training/nicaea_rescaled/training_data22/sv_training_wiener.npy
Loading /home/ucapnje/share/DeepMass/run_scripts/../picola_training/nicaea_rescaled/training_data23/sv_training_wiener.npy
Loading /home/ucapnje/share/DeepMass/run_scripts/../picola_training/nicaea_rescaled/training_data24/sv_training_wiener.npy
Loading /home/ucapnje/share/DeepMass/run_scripts/../picola_training/nicaea_rescaled/training_data25/sv_training_wiener.npy
Loading /home/ucapnje/share/DeepMass/run_scripts/../picola_training/nicaea_rescaled/training_data26/sv_training_wiener.npy
Loading /home/ucapnje/share/DeepMass/run_scripts/../picola_training/nicaea_rescaled/training_data27/sv_training_wiener.npy
Loading /home/ucapnje/share/DeepMass/run_scripts/../picola_training/nicaea_rescaled/training_data28/sv_training_wiener.npy
Loading /home/ucapnje/share/DeepMass/run_scripts/../picola_training/nicaea_rescaled/training_data29/sv_training_wiener.npy
Loading /home/ucapnje/share/DeepMass/run_scripts/../picola_training/nicaea_rescaled/training_data30/sv_training_wiener.npy
Loading /home/ucapnje/share/DeepMass/run_scripts/../picola_training/nicaea_rescaled/training_data31/sv_training_wiener.npy
Loading /home/ucapnje/share/DeepMass/run_scripts/../picola_training/nicaea_rescaled/training_data32/sv_training_wiener.npy
Loading /home/ucapnje/share/DeepMass/run_scripts/../picola_training/nicaea_rescaled/training_data33/sv_training_wiener.npy
Loading /home/ucapnje/share/DeepMass/run_scripts/../picola_training/nicaea_rescaled/training_data34/sv_training_wiener.npy
Loading /home/ucapnje/share/DeepMass/run_scripts/../picola_training/nicaea_rescaled/training_data35/sv_training_wiener.npy
Loading /home/ucapnje/share/DeepMass/run_scripts/../picola_training/nicaea_rescaled/training_data36/sv_training_wiener.npy
Loading /home/ucapnje/share/DeepMass/run_scripts/../picola_training/nicaea_rescaled/training_data37/sv_training_wiener.npy
Loading /home/ucapnje/share/DeepMass/run_scripts/../picola_training/nicaea_rescaled/training_data38/sv_training_wiener.npy
Loading /home/ucapnje/share/DeepMass/run_scripts/../picola_training/nicaea_rescaled/training_data39/sv_training_wiener.npy
Loading /home/ucapnje/share/DeepMass/run_scripts/../picola_training/nicaea_rescaled/training_data40/sv_training_wiener.npy
Loading /home/ucapnje/share/DeepMass/run_scripts/../picola_training/nicaea_rescaled/training_data41/sv_training_wiener.npy
Loading /home/ucapnje/share/DeepMass/run_scripts/../picola_training/nicaea_rescaled/training_data42/sv_training_wiener.npy
Loading /home/ucapnje/share/DeepMass/run_scripts/../picola_training/nicaea_rescaled/training_data43/sv_training_wiener.npy
Loading /home/ucapnje/share/DeepMass/run_scripts/../picola_training/nicaea_rescaled/training_data44/sv_training_wiener.npy
Loading /home/ucapnje/share/DeepMass/run_scripts/../picola_training/nicaea_rescaled/training_data45/sv_training_wiener.npy
Loading /home/ucapnje/share/DeepMass/run_scripts/../picola_training/nicaea_rescaled/training_data46/sv_training_wiener.npy
Loading /home/ucapnje/share/DeepMass/run_scripts/../picola_training/nicaea_rescaled/training_data47/sv_training_wiener.npy
Loading /home/ucapnje/share/DeepMass/run_scripts/../picola_training/nicaea_rescaled/training_data48/sv_training_wiener.npy
Loading /home/ucapnje/share/DeepMass/run_scripts/../picola_training/nicaea_rescaled/training_data49/sv_training_wiener.npy
Loading /home/ucapnje/share/DeepMass/run_scripts/../picola_training/nicaea_rescaled/training_data50/sv_training_wiener.npy
Loading /home/ucapnje/share/DeepMass/run_scripts/../picola_training/nicaea_rescaled/training_data51/sv_training_wiener.npy
Loading /home/ucapnje/share/DeepMass/run_scripts/../picola_training/nicaea_rescaled/training_data52/sv_training_wiener.npy
Loading /home/ucapnje/share/DeepMass/run_scripts/../picola_training/nicaea_rescaled/training_data53/sv_training_wiener.npy
Loading /home/ucapnje/share/DeepMass/run_scripts/../picola_training/nicaea_rescaled/training_data54/sv_training_wiener.npy
Loading /home/ucapnje/share/DeepMass/run_scripts/../picola_training/nicaea_rescaled/training_data55/sv_training_wiener.npy
Loading /home/ucapnje/share/DeepMass/run_scripts/../picola_training/nicaea_rescaled/training_data56/sv_training_wiener.npy
Loading /home/ucapnje/share/DeepMass/run_scripts/../picola_training/nicaea_rescaled/training_data57/sv_training_wiener.npy
4816.432703256607

Apply mask

Number of bad files = 503


Shuffle and take fraction of test data
Number of pixels sample = 131072000
pixels out of range (truth) = 5
pixels out of range (input/noisy) = 0
clean array bytes = 60685549568
noisy array bytes = 60685549568
Test loss = 5.155502e-05
Plotting data. Saving to: ../outputs/picola_script_outputs/picola_data.png
<generator object batch_generator at 0x2b73189d4fc0>
(223497, 256, 256, 1)
(8000, 256, 256, 1)
6984
training network wiener (no dropout) 


simple lr = 3e-05
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         (None, 256, 256, 1)       0         
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 256, 256, 32)      320       
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 256, 256, 32)      9248      
_________________________________________________________________
batch_normalization_1 (Batch (None, 256, 256, 32)      128       
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 256, 256, 32)      9248      
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 256, 256, 32)      9248      
_________________________________________________________________
batch_normalization_2 (Batch (None, 256, 256, 32)      128       
_________________________________________________________________
conv2d_5 (Conv2D)            (None, 256, 256, 1)       289       
=================================================================
Total params: 28,609
Trainable params: 28,481
Non-trainable params: 128
_________________________________________________________________
20 32 3e-05
Epoch 1/20
Using TensorFlow backend.
 - 667s - loss: 5.7859e-04 - val_loss: 5.2170e-05
Epoch 2/20
 - 661s - loss: 4.8976e-05 - val_loss: 5.0038e-05
Epoch 3/20
 - 662s - loss: 4.8841e-05 - val_loss: 5.0443e-05
Epoch 4/20
 - 662s - loss: 4.8800e-05 - val_loss: 4.8773e-05
Epoch 5/20
 - 663s - loss: 4.8811e-05 - val_loss: 5.0699e-05
Epoch 6/20
 - 662s - loss: 4.8659e-05 - val_loss: 5.1263e-05
Epoch 7/20
 - 661s - loss: 4.8643e-05 - val_loss: 4.8753e-05
Epoch 8/20
 - 662s - loss: 4.8641e-05 - val_loss: 4.9219e-05
Epoch 9/20
 - 662s - loss: 4.8708e-05 - val_loss: 5.0669e-05
Epoch 10/20
 - 662s - loss: 4.8513e-05 - val_loss: 5.0168e-05
Epoch 11/20
 - 659s - loss: 4.8494e-05 - val_loss: 4.8522e-05
Epoch 12/20
 - 659s - loss: 4.8571e-05 - val_loss: 4.8628e-05
Epoch 13/20
 - 659s - loss: 4.8516e-05 - val_loss: 5.3934e-05
Epoch 14/20
 - 659s - loss: 4.8608e-05 - val_loss: 5.2220e-05
Epoch 15/20
 - 659s - loss: 4.8504e-05 - val_loss: 4.8643e-05
Epoch 16/20
 - 659s - loss: 4.8544e-05 - val_loss: 4.8629e-05
Epoch 17/20
 - 659s - loss: 4.8507e-05 - val_loss: 5.2274e-05
Epoch 18/20
 - 660s - loss: 4.8544e-05 - val_loss: 4.7942e-05
Epoch 19/20
 - 659s - loss: 4.8511e-05 - val_loss: 4.9339e-05
Epoch 20/20
 - 659s - loss: 4.8552e-05 - val_loss: 4.8915e-05

simple lr = 1e-05
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_2 (InputLayer)         (None, 256, 256, 1)       0         
_________________________________________________________________
conv2d_6 (Conv2D)            (None, 256, 256, 32)      320       
_________________________________________________________________
conv2d_7 (Conv2D)            (None, 256, 256, 32)      9248      
_________________________________________________________________
batch_normalization_3 (Batch (None, 256, 256, 32)      128       
_________________________________________________________________
conv2d_8 (Conv2D)            (None, 256, 256, 32)      9248      
_________________________________________________________________
conv2d_9 (Conv2D)            (None, 256, 256, 32)      9248      
_________________________________________________________________
batch_normalization_4 (Batch (None, 256, 256, 32)      128       
_________________________________________________________________
conv2d_10 (Conv2D)           (None, 256, 256, 1)       289       
=================================================================
Total params: 28,609
Trainable params: 28,481
Non-trainable params: 128
_________________________________________________________________
20 32 1e-05
Epoch 1/20
 - 657s - loss: 0.0025 - val_loss: 0.0011
Epoch 2/20
 - 656s - loss: 1.7870e-04 - val_loss: 3.9045e-04
Epoch 3/20
 - 657s - loss: 5.2832e-05 - val_loss: 1.1289e-04
Epoch 4/20
 - 656s - loss: 4.9073e-05 - val_loss: 1.5166e-04
Epoch 5/20
 - 657s - loss: 4.8906e-05 - val_loss: 5.4125e-05
Epoch 6/20
 - 656s - loss: 4.8724e-05 - val_loss: 1.6238e-04
Epoch 7/20
 - 656s - loss: 4.8689e-05 - val_loss: 5.0761e-05
Epoch 8/20
 - 656s - loss: 4.8676e-05 - val_loss: 7.0542e-05
Epoch 9/20
 - 656s - loss: 4.8736e-05 - val_loss: 4.9365e-05
Epoch 10/20
 - 656s - loss: 4.8539e-05 - val_loss: 5.1561e-05
Epoch 11/20
 - 656s - loss: 4.8518e-05 - val_loss: 6.0679e-05
Epoch 12/20
 - 655s - loss: 4.8598e-05 - val_loss: 8.7520e-05
Epoch 13/20
 - 656s - loss: 4.8544e-05 - val_loss: 4.5620e-04
Epoch 14/20
 - 656s - loss: 4.8639e-05 - val_loss: 1.2991e-04
Epoch 15/20
 - 656s - loss: 4.8536e-05 - val_loss: 8.2572e-05
Epoch 16/20
 - 656s - loss: 4.8578e-05 - val_loss: 8.0063e-05
Epoch 17/20
 - 656s - loss: 4.8545e-05 - val_loss: 4.9639e-05
Epoch 18/20
 - 656s - loss: 4.8582e-05 - val_loss: 5.1734e-05
Epoch 19/20
 - 656s - loss: 4.8551e-05 - val_loss: 4.9264e-05
Epoch 20/20
 - 656s - loss: 4.8596e-05 - val_loss: 5.3472e-05
Epoch 1/20
Epoch 1/20

unet simple lr = 3e-05
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_3 (InputLayer)            (None, 256, 256, 1)  0                                            
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 256, 256, 32) 320         input_3[0][0]                    
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 256, 256, 32) 128         conv2d_11[0][0]                  
__________________________________________________________________________________________________
average_pooling2d_1 (AveragePoo (None, 128, 128, 32) 0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 128, 128, 64) 18496       average_pooling2d_1[0][0]        
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 128, 128, 64) 256         conv2d_12[0][0]                  
__________________________________________________________________________________________________
average_pooling2d_2 (AveragePoo (None, 64, 64, 64)   0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 64, 64, 128)  73856       average_pooling2d_2[0][0]        
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 64, 64, 128)  512         conv2d_13[0][0]                  
__________________________________________________________________________________________________
up_sampling2d_1 (UpSampling2D)  (None, 128, 128, 128 0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 128, 128, 192 0           batch_normalization_6[0][0]      
                                                                 up_sampling2d_1[0][0]            
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 128, 128, 192 768         concatenate_1[0][0]              
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, 128, 128, 64) 110656      batch_normalization_8[0][0]      
__________________________________________________________________________________________________
up_sampling2d_2 (UpSampling2D)  (None, 256, 256, 64) 0           conv2d_14[0][0]                  
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 256, 256, 96) 0           batch_normalization_5[0][0]      
                                                                 up_sampling2d_2[0][0]            
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 256, 256, 96) 384         concatenate_2[0][0]              
__________________________________________________________________________________________________
conv2d_15 (Conv2D)              (None, 256, 256, 32) 27680       batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv2d_16 (Conv2D)              (None, 256, 256, 1)  33          conv2d_15[0][0]                  
==================================================================================================
Total params: 233,089
Trainable params: 232,065
Non-trainable params: 1,024
__________________________________________________________________________________________________
20 32 3e-05
Epoch 1/20
 - 1092s - loss: 1.8135e-04 - val_loss: 5.8049e-05
Epoch 2/20
 - 1088s - loss: 5.7026e-05 - val_loss: 5.7234e-05
Epoch 3/20
 - 1089s - loss: 5.6824e-05 - val_loss: 5.6924e-05
Epoch 4/20
 - 1088s - loss: 5.3282e-05 - val_loss: 5.2758e-05
Epoch 5/20
 - 1089s - loss: 5.2923e-05 - val_loss: 5.2981e-05
Epoch 6/20
 - 1088s - loss: 5.2754e-05 - val_loss: 5.2634e-05
Epoch 7/20
 - 1089s - loss: 5.0334e-05 - val_loss: 4.9707e-05
Epoch 8/20
 - 1088s - loss: 4.8899e-05 - val_loss: 5.0391e-05
Epoch 9/20
 - 1089s - loss: 4.8954e-05 - val_loss: 4.9257e-05
Epoch 10/20
 - 1089s - loss: 4.8746e-05 - val_loss: 4.9001e-05
Epoch 11/20
 - 1088s - loss: 4.8716e-05 - val_loss: 4.8656e-05
Epoch 12/20
 - 1088s - loss: 4.8788e-05 - val_loss: 4.9052e-05
Epoch 13/20
 - 1088s - loss: 4.8732e-05 - val_loss: 4.8750e-05
Epoch 14/20
 - 1088s - loss: 4.8809e-05 - val_loss: 4.8970e-05
Epoch 15/20
 - 1089s - loss: 4.8697e-05 - val_loss: 4.8713e-05
Epoch 16/20
 - 1088s - loss: 4.8726e-05 - val_loss: 4.8276e-05
Epoch 17/20
 - 1089s - loss: 4.8686e-05 - val_loss: 4.8487e-05
Epoch 18/20
 - 1088s - loss: 4.8705e-05 - val_loss: 4.8366e-05
Epoch 19/20
 - 1088s - loss: 4.8658e-05 - val_loss: 4.8772e-05
Epoch 20/20
 - 1089s - loss: 4.8672e-05 - val_loss: 4.8874e-05

unet simple lr = 1e-05
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_4 (InputLayer)            (None, 256, 256, 1)  0                                            
__________________________________________________________________________________________________
conv2d_17 (Conv2D)              (None, 256, 256, 32) 320         input_4[0][0]                    
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 256, 256, 32) 128         conv2d_17[0][0]                  
__________________________________________________________________________________________________
average_pooling2d_3 (AveragePoo (None, 128, 128, 32) 0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
conv2d_18 (Conv2D)              (None, 128, 128, 64) 18496       average_pooling2d_3[0][0]        
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 128, 128, 64) 256         conv2d_18[0][0]                  
__________________________________________________________________________________________________
average_pooling2d_4 (AveragePoo (None, 64, 64, 64)   0           batch_normalization_11[0][0]     
__________________________________________________________________________________________________
conv2d_19 (Conv2D)              (None, 64, 64, 128)  73856       average_pooling2d_4[0][0]        
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 64, 64, 128)  512         conv2d_19[0][0]                  
__________________________________________________________________________________________________
up_sampling2d_3 (UpSampling2D)  (None, 128, 128, 128 0           batch_normalization_12[0][0]     
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 128, 128, 192 0           batch_normalization_11[0][0]     
                                                                 up_sampling2d_3[0][0]            
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 128, 128, 192 768         concatenate_3[0][0]              
__________________________________________________________________________________________________
conv2d_20 (Conv2D)              (None, 128, 128, 64) 110656      batch_normalization_13[0][0]     
__________________________________________________________________________________________________
up_sampling2d_4 (UpSampling2D)  (None, 256, 256, 64) 0           conv2d_20[0][0]                  
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 256, 256, 96) 0           batch_normalization_10[0][0]     
                                                                 up_sampling2d_4[0][0]            
__________________________________________________________________________________________________
batch_normalization_14 (BatchNo (None, 256, 256, 96) 384         concatenate_4[0][0]              
__________________________________________________________________________________________________
conv2d_21 (Conv2D)              (None, 256, 256, 32) 27680       batch_normalization_14[0][0]     
__________________________________________________________________________________________________
conv2d_22 (Conv2D)              (None, 256, 256, 1)  33          conv2d_21[0][0]                  
==================================================================================================
Total params: 233,089
Trainable params: 232,065
Non-trainable params: 1,024
__________________________________________________________________________________________________
20 32 1e-05
Epoch 1/20
 - 1085s - loss: 3.1160e-04 - val_loss: 5.8120e-05
Epoch 2/20
 - 1082s - loss: 5.3714e-05 - val_loss: 5.3787e-05
Epoch 3/20
 - 1082s - loss: 5.2705e-05 - val_loss: 5.2975e-05
Epoch 4/20
 - 1081s - loss: 5.2566e-05 - val_loss: 5.2536e-05
Epoch 5/20
 - 1082s - loss: 5.2552e-05 - val_loss: 5.2386e-05
Epoch 6/20
 - 1082s - loss: 5.2393e-05 - val_loss: 5.2422e-05
Epoch 7/20
 - 1082s - loss: 5.2374e-05 - val_loss: 5.2446e-05
Epoch 8/20
 - 1083s - loss: 5.2370e-05 - val_loss: 5.2908e-05
Epoch 9/20
 - 1081s - loss: 5.2431e-05 - val_loss: 5.2789e-05
Epoch 10/20
 - 1082s - loss: 5.2233e-05 - val_loss: 5.2547e-05
Epoch 11/20
 - 1082s - loss: 5.2210e-05 - val_loss: 5.2236e-05
Epoch 12/20
 - 1081s - loss: 5.2286e-05 - val_loss: 5.2221e-05
Epoch 13/20
 - 1082s - loss: 5.2229e-05 - val_loss: 5.2341e-05
Epoch 14/20
 - 1082s - loss: 5.2318e-05 - val_loss: 5.2625e-05
Epoch 15/20
 - 1081s - loss: 5.2213e-05 - val_loss: 5.2344e-05
Epoch 16/20
 - 1081s - loss: 5.2251e-05 - val_loss: 5.1838e-05
Epoch 17/20
 - 1081s - loss: 5.2216e-05 - val_loss: 5.2012e-05
Epoch 18/20
 - 1082s - loss: 5.2251e-05 - val_loss: 5.1608e-05
Epoch 19/20
 - 1082s - loss: 5.2217e-05 - val_loss: 5.2325e-05
Epoch 20/20
 - 1082s - loss: 5.2259e-05 - val_loss: 5.2501e-05

unet deep lr = 3e-05
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_5 (InputLayer)            (None, 256, 256, 1)  0                                            
__________________________________________________________________________________________________
conv2d_23 (Conv2D)              (None, 256, 256, 32) 320         input_5[0][0]                    
__________________________________________________________________________________________________
batch_normalization_15 (BatchNo (None, 256, 256, 32) 128         conv2d_23[0][0]                  
__________________________________________________________________________________________________
average_pooling2d_5 (AveragePoo (None, 128, 128, 32) 0           batch_normalization_15[0][0]     
__________________________________________________________________________________________________
conv2d_24 (Conv2D)              (None, 128, 128, 64) 18496       average_pooling2d_5[0][0]        
__________________________________________________________________________________________________
batch_normalization_16 (BatchNo (None, 128, 128, 64) 256         conv2d_24[0][0]                  
__________________________________________________________________________________________________
average_pooling2d_6 (AveragePoo (None, 64, 64, 64)   0           batch_normalization_16[0][0]     
__________________________________________________________________________________________________
conv2d_25 (Conv2D)              (None, 64, 64, 128)  73856       average_pooling2d_6[0][0]        
__________________________________________________________________________________________________
batch_normalization_17 (BatchNo (None, 64, 64, 128)  512         conv2d_25[0][0]                  
__________________________________________________________________________________________________
average_pooling2d_7 (AveragePoo (None, 32, 32, 128)  0           batch_normalization_17[0][0]     
__________________________________________________________________________________________________
conv2d_26 (Conv2D)              (None, 32, 32, 128)  147584      average_pooling2d_7[0][0]        
__________________________________________________________________________________________________
batch_normalization_18 (BatchNo (None, 32, 32, 128)  512         conv2d_26[0][0]                  
__________________________________________________________________________________________________
up_sampling2d_5 (UpSampling2D)  (None, 64, 64, 128)  0           batch_normalization_18[0][0]     
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 64, 64, 256)  0           batch_normalization_17[0][0]     
                                                                 up_sampling2d_5[0][0]            
__________________________________________________________________________________________________
batch_normalization_19 (BatchNo (None, 64, 64, 256)  1024        concatenate_5[0][0]              
__________________________________________________________________________________________________
conv2d_27 (Conv2D)              (None, 64, 64, 64)   147520      batch_normalization_19[0][0]     
__________________________________________________________________________________________________
up_sampling2d_6 (UpSampling2D)  (None, 128, 128, 64) 0           conv2d_27[0][0]                  
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 128, 128, 128 0           batch_normalization_16[0][0]     
                                                                 up_sampling2d_6[0][0]            
__________________________________________________________________________________________________
batch_normalization_20 (BatchNo (None, 128, 128, 128 512         concatenate_6[0][0]              
__________________________________________________________________________________________________
conv2d_28 (Conv2D)              (None, 128, 128, 64) 73792       batch_normalization_20[0][0]     
__________________________________________________________________________________________________
up_sampling2d_7 (UpSampling2D)  (None, 256, 256, 64) 0           conv2d_28[0][0]                  
__________________________________________________________________________________________________
concatenate_7 (Concatenate)     (None, 256, 256, 96) 0           batch_normalization_15[0][0]     
                                                                 up_sampling2d_7[0][0]            
__________________________________________________________________________________________________
batch_normalization_21 (BatchNo (None, 256, 256, 96) 384         concatenate_7[0][0]              
__________________________________________________________________________________________________
conv2d_29 (Conv2D)              (None, 256, 256, 32) 27680       batch_normalization_21[0][0]     
__________________________________________________________________________________________________
conv2d_30 (Conv2D)              (None, 256, 256, 1)  33          conv2d_29[0][0]                  
==================================================================================================
Total params: 492,609
Trainable params: 490,945
Non-trainable params: 1,664
__________________________________________________________________________________________________
20 32 3e-05
Epoch 1/20
 - 1110s - loss: 2.8476e-04 - val_loss: 5.8128e-05
Epoch 2/20
 - 1108s - loss: 5.5608e-05 - val_loss: 5.3312e-05
Epoch 3/20
 - 1107s - loss: 5.2675e-05 - val_loss: 5.3083e-05
Epoch 4/20
 - 1108s - loss: 5.2568e-05 - val_loss: 5.2756e-05
Epoch 5/20
 - 1108s - loss: 5.2535e-05 - val_loss: 5.2311e-05
Epoch 6/20
 - 1107s - loss: 5.2349e-05 - val_loss: 5.2318e-05
Epoch 7/20
 - 1107s - loss: 5.2314e-05 - val_loss: 5.2502e-05
Epoch 8/20
 - 1107s - loss: 5.2296e-05 - val_loss: 5.2948e-05
Epoch 9/20
 - 1107s - loss: 5.2352e-05 - val_loss: 5.2961e-05
Epoch 10/20
 - 1108s - loss: 5.0526e-05 - val_loss: 4.8701e-05
Epoch 11/20
 - 1108s - loss: 4.8306e-05 - val_loss: 4.8545e-05
Epoch 12/20
 - 1108s - loss: 4.8375e-05 - val_loss: 4.8599e-05
Epoch 13/20
 - 1108s - loss: 4.8314e-05 - val_loss: 4.8707e-05
Epoch 14/20
 - 1107s - loss: 4.8395e-05 - val_loss: 4.8806e-05
Epoch 15/20
 - 1108s - loss: 4.8287e-05 - val_loss: 4.9000e-05
Epoch 16/20
 - 1107s - loss: 4.8318e-05 - val_loss: 4.8536e-05
Epoch 17/20
 - 1108s - loss: 4.8277e-05 - val_loss: 4.8176e-05
Epoch 18/20
 - 1108s - loss: 4.8305e-05 - val_loss: 4.7737e-05
Epoch 19/20
 - 1107s - loss: 4.8265e-05 - val_loss: 4.8464e-05
Epoch 20/20
 - 1108s - loss: 4.8300e-05 - val_loss: 4.8774e-05

unet deep lr = 1e-05
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_6 (InputLayer)            (None, 256, 256, 1)  0                                            
__________________________________________________________________________________________________
conv2d_31 (Conv2D)              (None, 256, 256, 32) 320         input_6[0][0]                    
__________________________________________________________________________________________________
batch_normalization_22 (BatchNo (None, 256, 256, 32) 128         conv2d_31[0][0]                  
__________________________________________________________________________________________________
average_pooling2d_8 (AveragePoo (None, 128, 128, 32) 0           batch_normalization_22[0][0]     
__________________________________________________________________________________________________
conv2d_32 (Conv2D)              (None, 128, 128, 64) 18496       average_pooling2d_8[0][0]        
__________________________________________________________________________________________________
batch_normalization_23 (BatchNo (None, 128, 128, 64) 256         conv2d_32[0][0]                  
__________________________________________________________________________________________________
average_pooling2d_9 (AveragePoo (None, 64, 64, 64)   0           batch_normalization_23[0][0]     
__________________________________________________________________________________________________
conv2d_33 (Conv2D)              (None, 64, 64, 128)  73856       average_pooling2d_9[0][0]        
__________________________________________________________________________________________________
batch_normalization_24 (BatchNo (None, 64, 64, 128)  512         conv2d_33[0][0]                  
__________________________________________________________________________________________________
average_pooling2d_10 (AveragePo (None, 32, 32, 128)  0           batch_normalization_24[0][0]     
__________________________________________________________________________________________________
conv2d_34 (Conv2D)              (None, 32, 32, 128)  147584      average_pooling2d_10[0][0]       
__________________________________________________________________________________________________
batch_normalization_25 (BatchNo (None, 32, 32, 128)  512         conv2d_34[0][0]                  
__________________________________________________________________________________________________
up_sampling2d_8 (UpSampling2D)  (None, 64, 64, 128)  0           batch_normalization_25[0][0]     
__________________________________________________________________________________________________
concatenate_8 (Concatenate)     (None, 64, 64, 256)  0           batch_normalization_24[0][0]     
                                                                 up_sampling2d_8[0][0]            
__________________________________________________________________________________________________
batch_normalization_26 (BatchNo (None, 64, 64, 256)  1024        concatenate_8[0][0]              
__________________________________________________________________________________________________
conv2d_35 (Conv2D)              (None, 64, 64, 64)   147520      batch_normalization_26[0][0]     
__________________________________________________________________________________________________
up_sampling2d_9 (UpSampling2D)  (None, 128, 128, 64) 0           conv2d_35[0][0]                  
__________________________________________________________________________________________________
concatenate_9 (Concatenate)     (None, 128, 128, 128 0           batch_normalization_23[0][0]     
                                                                 up_sampling2d_9[0][0]            
__________________________________________________________________________________________________
batch_normalization_27 (BatchNo (None, 128, 128, 128 512         concatenate_9[0][0]              
__________________________________________________________________________________________________
conv2d_36 (Conv2D)              (None, 128, 128, 64) 73792       batch_normalization_27[0][0]     
__________________________________________________________________________________________________
up_sampling2d_10 (UpSampling2D) (None, 256, 256, 64) 0           conv2d_36[0][0]                  
__________________________________________________________________________________________________
concatenate_10 (Concatenate)    (None, 256, 256, 96) 0           batch_normalization_22[0][0]     
                                                                 up_sampling2d_10[0][0]           
__________________________________________________________________________________________________
batch_normalization_28 (BatchNo (None, 256, 256, 96) 384         concatenate_10[0][0]             
__________________________________________________________________________________________________
conv2d_37 (Conv2D)              (None, 256, 256, 32) 27680       batch_normalization_28[0][0]     
__________________________________________________________________________________________________
conv2d_38 (Conv2D)              (None, 256, 256, 1)  33          conv2d_37[0][0]                  
==================================================================================================
Total params: 492,609
Trainable params: 490,945
Non-trainable params: 1,664
__________________________________________________________________________________________________
20 32 1e-05
Epoch 1/20
 - 1115s - loss: 4.9278e-04 - val_loss: 5.4455e-05
Epoch 2/20
 - 1112s - loss: 5.0352e-05 - val_loss: 5.0266e-05
Epoch 3/20
 - 1112s - loss: 4.9313e-05 - val_loss: 4.9544e-05
Epoch 4/20
 - 1112s - loss: 4.9099e-05 - val_loss: 4.9040e-05
Epoch 5/20
 - 1112s - loss: 4.9029e-05 - val_loss: 4.8872e-05
Epoch 6/20
 - 1112s - loss: 4.8827e-05 - val_loss: 4.8866e-05
Epoch 7/20
 - 1113s - loss: 4.8781e-05 - val_loss: 4.8831e-05
Epoch 8/20
 - 1112s - loss: 4.8753e-05 - val_loss: 4.9082e-05
Epoch 9/20
 - 1112s - loss: 4.8801e-05 - val_loss: 4.9150e-05
Epoch 10/20
 - 1112s - loss: 4.8585e-05 - val_loss: 4.8892e-05
Epoch 11/20
 - 1112s - loss: 4.8551e-05 - val_loss: 4.8563e-05
Epoch 12/20
 - 1112s - loss: 4.8614e-05 - val_loss: 4.8485e-05
Epoch 13/20
 - 1112s - loss: 4.8548e-05 - val_loss: 4.8630e-05
Epoch 14/20
 - 1112s - loss: 4.8627e-05 - val_loss: 4.8837e-05
Epoch 15/20
 - 1112s - loss: 4.8511e-05 - val_loss: 4.8572e-05
Epoch 16/20
 - 1112s - loss: 4.8542e-05 - val_loss: 4.8137e-05
Epoch 17/20
 - 1112s - loss: 4.8498e-05 - val_loss: 4.8292e-05
Epoch 18/20
 - 1112s - loss: 4.8528e-05 - val_loss: 4.7936e-05
Epoch 19/20
 - 1112s - loss: 4.8489e-05 - val_loss: 4.8634e-05
Epoch 20/20
 - 1112s - loss: 4.8526e-05 - val_loss: 4.8710e-05
Epoch 1/20
